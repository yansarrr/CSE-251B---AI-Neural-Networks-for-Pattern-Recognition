jil232@dsmlp-jupyter-jil232:~/private/cse151b251b-wi25-pa3-cse151b_251b_lstm$ python3 main.py configs/lstm_seq128_no_teacher_force_config.yaml
ENCODED TEXT DATA
CREATED SEQUENCES
Using sequence length: 128
PERFORMED TRAIN/VAL/TEST SPLIT
X_train: torch.Size([892212, 128]), y_train: torch.Size([892212])
X_val: torch.Size([111526, 128]), y_val: torch.Size([111526])
X_test: torch.Size([111528, 128]), y_test: torch.Size([111528])
Using LSTM Without Teacher Force.
Epoch 1: 100%|█████████████████████████████████████| 13941/13941 [12:33<00:00, 18.50it/s]
Epoch 1: Train Loss = 3.3199, Val Loss = 3.3097
Epoch 2: 100%|█████████████████████████████████████| 13941/13941 [12:34<00:00, 18.48it/s]
Epoch 2: Train Loss = 3.3159, Val Loss = 3.3106
Epoch 3: 100%|█████████████████████████████████████| 13941/13941 [12:13<00:00, 19.01it/s]
Epoch 3: Train Loss = 3.3153, Val Loss = 3.3097
Early stopping after 3 epochs
Epoch 4: 100%|█████████████████████████████████████| 13941/13941 [12:47<00:00, 18.17it/s]
Epoch 4: Train Loss = 3.3152, Val Loss = 3.3092
Epoch 5: 100%|█████████████████████████████████████| 13941/13941 [12:38<00:00, 18.37it/s]
Epoch 5: Train Loss = 3.3149, Val Loss = 3.3087
Epoch 6: 100%|█████████████████████████████████████| 13941/13941 [12:09<00:00, 19.11it/s]
Epoch 6: Train Loss = 3.3148, Val Loss = 3.3088
Epoch 7: 100%|█████████████████████████████████████| 13941/13941 [12:35<00:00, 18.45it/s]
Epoch 7: Train Loss = 3.3147, Val Loss = 3.3086
Epoch 8: 100%|█████████████████████████████████████| 13941/13941 [12:44<00:00, 18.25it/s]
Epoch 8: Train Loss = 3.3146, Val Loss = 3.3084
Epoch 9: 100%|█████████████████████████████████████| 13941/13941 [12:26<00:00, 18.68it/s]
Epoch 9: Train Loss = 3.3145, Val Loss = 3.3082
Epoch 10: 100%|████████████████████████████████████| 13941/13941 [12:31<00:00, 18.55it/s]
Epoch 10: Train Loss = 3.3145, Val Loss = 3.3084
test loss: 3.3143451782867817
Epoch 10: 100%|████████████████████████████████████| 13941/13941 [12:31<00:00, 18.55it/s]
Epoch 10: Train Loss = 3.3145, Val Loss = 3.3084
test loss: 3.3143451782867817